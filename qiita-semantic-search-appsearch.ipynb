{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# このJupyter Notebookの使い方\n",
    "これはレシピ集として作成されています。最初に、初期設定セクションにて利用するElasticsearch環境やOpenAI環境、HuggingFace環境の接続情報を設定します。その後は、実行したいセクションA. B. C. ..から始めて順番にコマンドを実行してください。\n",
    "そのためにセクション間で重複したコードが繰り返しあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.初期設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリの有効化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q elasticsearch requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os, json\n",
    "import requests\n",
    "from getpass import getpass\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.helpers import bulk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearchの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectApiResponse({'name': 'instance-0000000054', 'cluster_name': '507a2cf6ba204071943512e0537eee58', 'cluster_uuid': 'oF-xDLtXRCet87gRuM3eJg', 'version': {'number': '8.11.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '76013fa76dcbf144c886990c6290715f5dc2ae20', 'build_date': '2023-12-05T10:03:47.729926671Z', 'build_snapshot': False, 'lucene_version': '9.8.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})\n"
     ]
    }
   ],
   "source": [
    "ELASTIC_CLOUD_ID = getpass(\"Elastic deployment Cloud ID\")\n",
    "ELASTIC_API_KEY = getpass(\"Elastic deployment API Key\")\n",
    "if ELASTIC_CLOUD_ID == '':\n",
    "  ELASTIC_URL = getpass(\"Elastic deployment URL. No need if Cloud ID is provided.\")\n",
    "if ELASTIC_API_KEY == '':\n",
    "  ELASTIC_USER = getpass(\"Elastic user. No need if API key is provided.\")\n",
    "  ELASTIC_PASSWORD = getpass(\"Elastic password. No need if API key is provided.\")\n",
    "\n",
    "if ELASTIC_CLOUD_ID != '' and ELASTIC_API_KEY != '':\n",
    "  es = Elasticsearch(\n",
    "    cloud_id=ELASTIC_CLOUD_ID,\n",
    "    api_key=ELASTIC_API_KEY,\n",
    "    request_timeout=300\n",
    "  )\n",
    "elif ELASTIC_URL != '' and ELASTIC_USER != '' and ELASTIC_PASSWORD != '':\n",
    "  es = Elasticsearch(\n",
    "    hosts = ELASTIC_URL,\n",
    "    basic_auth=(ELASTIC_USER, ELASTIC_PASSWORD),\n",
    "    request_timeout=300\n",
    "  )\n",
    "elif ELASTIC_URL != '' and ELASTIC_USER == '':\n",
    "  es = Elasticsearch(\n",
    "    hosts = ELASTIC_URL,\n",
    "    # request_timeout=300,\n",
    "    request_timeout=300\n",
    "  )\n",
    "else:\n",
    "  print(\"env needs to set either ELASTIC_CLOUD_ID or ELASTIC_URL\")\n",
    "\n",
    "\n",
    "pprint(es.info()) # should return cluster info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearchで使うインデックス名の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qiita-blog-appsearch'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_NAME=input(\"Elasticsearchのインデックスの名前 (空入力はqiita-blog-appsearchになります):\")\n",
    "if INDEX_NAME == '':\n",
    "    INDEX_NAME='qiita-blog-appsearch'\n",
    "INDEX_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.検索ドキュメントのセットアップ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qiita記事のダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n",
      "total_count: 72\n"
     ]
    }
   ],
   "source": [
    "# h = {'Authorization': 'Bearer xxxx'} # ユーザ認証する場合\n",
    "h = {}\n",
    "url = \"https://qiita.com/api/v2/items?\"\n",
    "\n",
    "# tag別に記事をPAGEだけ繰り返し取得\n",
    "query = \"&query=org%3Aelasticsearch_japan\"\n",
    "# 検索で指定した期間内に作成された記事数を取得\n",
    "res = requests.get(url=url + query, headers=h)\n",
    "# サーバーからの応答\n",
    "print(res.status_code, res.reason)\n",
    "# print(\"指定しているタグ: \" + tag_name)\n",
    "total_count = int(res.headers['Total-Count'])\n",
    "print(\"total_count: \" + str(total_count))\n",
    "\n",
    "page = f\"page=1&per_page={total_count}\"\n",
    "os.makedirs(\"qiita-downloads\", exist_ok=True)\n",
    "res = requests.get(url=url + page + query, headers=h)\n",
    "documents = json.loads(res.text)\n",
    "for doc in documents:\n",
    "    search_doc = {\n",
    "        \"title\": doc[\"title\"],\n",
    "        \"url\": doc[\"url\"],\n",
    "        \"body\": doc[\"body\"],\n",
    "        \"tags\": doc[\"tags\"],\n",
    "        \"created_at\": doc[\"created_at\"],\n",
    "        \"updated_at\": doc[\"updated_at\"],\n",
    "        \"id\": doc[\"id\"],\n",
    "        \"likes_count\": doc[\"likes_count\"],\n",
    "        \"reactions_count\": doc[\"reactions_count\"],\n",
    "        \"stocks_count\": doc[\"stocks_count\"],\n",
    "        \"page_views_count\": doc[\"page_views_count\"],\n",
    "        \"organization_url_name\": doc[\"organization_url_name\"],\n",
    "        \"comments_count\": doc[\"comments_count\"]\n",
    "    }\n",
    "    title = doc[\"title\"].replace('/', '_')\n",
    "    filename = \"./qiita-downloads\" + \"/\" + title + \".json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(search_doc, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document count: 72\n",
      "{'title': 'Elastic Stack 8.0 の NLP で日本語センチメント分析を試してみた - 後編', 'url': 'https://qiita.com/ijokarumawak@github/items/6cc714060090160cf2d5', 'body': '先日 [Elastic Stack 8.0 の NLP で日本語センチメント分析を試してみた](https://qiita.com/ijokarumawak@github/items/9b0c2d650536488718a5) を書いたところ、「これ、ちゃんと日本語で処理できるのかな？中の動きが知りたい」とコメントいただきました。確かに、モデル側では fugashi などを使っているのに Elasticsearch 側では使ってないはずですね。\\n\\n今回は Elastic Stack 8.0.1 を使って、 inference で判定させるテキストをどうやって tokenize しているかを調査してみました。\\n\\n勿体ぶらずにまずは結論から。\\n\\n## 結論: Elastic Stack 8.0.1 時点では、日本語は Unigram で扱われている\\n\\n全体の処理の流れを Tokenizer を中心に整理してみました。\\n\\n![nlp-tokenizer.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/20515/3f752cb2-c072-badf-a117-075bdc4971b8.png)\\n\\n\\n1. ***学習フェーズ*** 事前に学習する部分。 BertForSequenceClassification というアーキテクチャの一部で、入力文字列を機械学習で扱うために変換する Transformer 。学習の成果物として **モデル** と **ボキャブラリ** ができます。\\n1. Eland を使って学習済みのモデルとボキャブラリを Hugging Face から Elasticsearch インデックスにインポートします。\\n1. ***判定フェーズ*** こちらが今回のテーマ、 Elastic Stack で分析対象の文字列をトークン分割している箇所です。分割した文字列とモデル ID を指定してネイティブな Pytorch プロセスを実行します。\\n\\n学習フェーズと判定フェーズでどのようにトークン分割しているかみてみましょう。\\n\\n## 学習フェーズ\\n\\n日本語を扱う場合、 [BertJapaneseTokenizer](https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer) があり、Hugging Face の一部として利用可能です。これは、[2019年12月、東北大学乾研究室からの成果](https://www.nlp.ecei.tohoku.ac.jp/news-release/3284/)。素晴らしいですね！\\n\\n形態素解析の MeCab を利用していて、意味のある単語レベルで分割して学習を行うことができます。\\n\\n```\\n例: \"小学校のころは守備が嫌いだった\"\\n => \"小学校\", \"ころ\", \"守備\", \"嫌い\"\\n```\\n(注: あくまでイメージです)\\n\\n学習の成果物として作成される vocabulary には 32,000 の語彙リストがあります。これで各語彙を数値に変換し、文章をベクトルとして扱えるものと思われます。\\n\\n## 判定フェーズ\\n\\n判定フェーズは Elastic Stack 側で実行される訳ですが、入力文字列を同じようにトークンに分割する必要があります。ver 8.0.1 時点では [BertTokenizer](https://github.com/elastic/elasticsearch/blob/v8.0.1/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/nlp/tokenizers/BertTokenizer.java) しかありません。\\n\\nBertTokenizer はまず [BasicTokenizer](https://github.com/elastic/elasticsearch/blob/v8.0.1/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/nlp/tokenizers/BasicTokenizer.java) で入力文字列を粗く分割したうえで、 [WordPieceTokenizer](https://github.com/elastic/elasticsearch/blob/v8.0.1/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/nlp/tokenizers/WordPieceTokenizer.java) で vocabulary の語彙でさらに分割します。\\n\\nVocabulary には 32,000 の語彙がある訳ですが、前編で利用したモデルでは次のような語彙がありました:\\n\\n```\\n\"[PAD]\",\\n\"[UNK]\",\\n\"[CLS]\",\\n\"[SEP]\",\\n\"[MASK]\",\\n\"の\",\\n\"、\",\\n\"に\",\\n\"。\",\\n\"は\",\\n…(中略)\\n\"##起こ\",\\n\"守備\",\\n\"小学\",\\n\"ころ\",\\n\"テン\",\\n\"イラスト\",\\n\"##if\",\\n\"規則\",\\n\"ページ\",\\n\"オル\",\\n\"上昇\",\\n\"境界\",\\n\"[\",\\n\"吸収\",\\n\"略称\",\\n\"##天皇\",\\n\"有力\",\\n\"##ながら\",\\n\"尽\",\\n\"滑\",\\n\"コレ\",\\n\"護衛\",\\n\"##ソード\",\\n\"チャート\",\\n(など)\\n```\\n\\nBasicTokenizer で分割されたトークンは、WordPieceTokenizer により、このリストとぶつけられて、語彙があればトークンの ID が付与され、なければ \"[UNK]\" (Unknown) となります。\\n\\nWordPieceTokenizer ではすでにトークンに分割されていることが前提なので、重要なのはその前処理にあたる BasicTokenizer です。\\n\\n## BasicTokenizer の実装\\n\\nBesicTokenizer は単純に入力文字をスペースや区切り文字で分割します。日本語はどう扱うかというと、 [Unigram に分割](https://github.com/elastic/elasticsearch/blob/v8.0.1/x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/nlp/tokenizers/BasicTokenizer.java#L194) します。\\n\\n```\\n例: \"小学校のころは守備が嫌いだった\"\\n => \"小\", \"学\", \"校\", \"の\", \"こ\", \"ろ\", \"は\", \"守\", \"備\", \"が\", \"嫌\", \"い\", \"だ\", \"っ\", \"た\"\\n```\\n\\n\\n## BertTokenizer のテスト\\n\\nBasicTokenizer と WordPieceTokenizer の動きを踏まえて、簡単な BertTokenizer のテストを実行してみました。\\n\\n```java\\n    public void testJapaneseTokens() {\\n        BertTokenizer tokenizer = BertTokenizer.builder(List.of(\\n                \"[UNK]\",\\n                \"[CLS]\",\\n                \"[SEP]\",\\n                \"[MASK]\",\\n                \"守備\",\\n                \"守\",\\n                \"小学生\"\\n            ), Tokenization.createDefault())\\n            .setDoLowerCase(false)\\n            .setWithSpecialTokens(true)\\n            // これは今のところ API ではオンオフできない\\n            .setDoTokenizeCjKChars(true)\\n            .build();\\n\\n        TokenizationResult.Tokenization result = tokenizer.tokenize(\"小学校のころは守備が嫌いだった\");\\n        String[] tokens = result.getTokens();\\n        System.out.println(List.of(tokens));\\n    }\\n```\\n\\n結果は次の通りです。 Vocabulary のリストには `守備` や `小学生` といった語彙がありますが、 BasicTokenizer では日本語を一文字に分割してしまうので、 `守` しか残りません。\\n\\n```\\n[[CLS], [UNK], [UNK], [UNK], [UNK], 守, [UNK], [UNK], [UNK], [UNK], [SEP]]\\n```\\n\\n## 日本語でも使えると言えるのか?\\n\\n前編の記事では二つの文章を判定させていました:\\n\\n- \"あちきの ML ノードは非力すぎ\" 94% ネガティブ\\n- \"Elastic Stack 8.0 で格段に NLP が簡単になった！\" 91% ポジティブ\\n\\nあくまで状況証拠による仮説ですが、利用したモデルの vocabulary には \"非\" や \"力\"、\"簡\" や \"単\" といった一文字が存在しました。このため、ある程度の精度でモデルを利用できていると思われます。\\n\\n単純に Unigram としての判定ではなく、意味のあるトークンで判定できた方が精度は高まるでしょう。\\n\\n## 今後の Tokenizer の動向に期待!\\n\\n今回調査したのは ver 8.0.1 でした。 `rg.elasticsearch.xpack.ml.inference.nlp.tokenizers` パッケージはこんな感じです。\\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/20515/6e6a907c-5d8c-7532-a06a-0482f81fa3b7.png)\\n\\n本日時点 (2022-03-04) の master ブランチを見ると:\\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/20515/0fce6763-a1b3-a818-202e-f8012f98791b.png)\\n\\n充実してきていますね。コミットの履歴を見ると、 [Tokenizer の実装を Lucene のに切り替えていこう](https://github.com/elastic/elasticsearch/pull/82870) という動きも見られます。今は限定的ですが、今後 Kuromoji や Sudachi なども inference の流れの Tokenizer で使えるようになると良いですね！\\n\\n## どうやってモデル実行しているの?\\n\\nElasticsearch は Java で実装されている訳ですが、モデルを使って inference を実施するのは別のネイティブプロセスが動いています。冒頭に登場したフロー図の `pytorch_inference` がそれで、 CPP で実装されています。 pytorch_inference の入出力はこちらの[テストプログラム](https://github.com/elastic/ml-cpp/blob/v8.0.1/bin/pytorch_inference/examples/sentiment_analysis/test_run.json)がイメージしやすかったです。\\n\\n```\\n\\t{\\n\\t\\t\"source_text\": \"The movie was awesome!!\",\\n\\t\\t\"input\": {\\n\\t\\t\\t\"request_id\": \"two\",\\n\\t\\t\\t\"tokens\": [[101, 1996, 3185, 2001, 12476, 999, 999, 102]],\\n\\t\\t\\t\"arg_1\": [[1, 1, 1, 1, 1, 1, 1, 1]]\\n\\t\\t},\\n\\t\\t\"expected_output\": {\"request_id\": \"two\", \"inference\": [[[-4.2720, 4.6515]]]}\\n\\t}\\n```\\n\\n`source_text` を `input` の `tokens` に変換するのを、本記事で紹介した `BertTokenizer` で Java 側で実施してから pytorch_inference に渡しているようです。\\n\\n## まとめ\\n今回は前編のブログ記事を見てくれた方からの質問に背中を押され、色々とソースコードを見たりして得られるものが多かったです。関連するソースコードへのリンクも要所要所で貼ったので、ご興味がある方はぜひ Elasticsearch のソースコードも見てみてください :)\\n', 'tags': ['NLP', 'Elasticsearch', 'Kibana'], 'created_at': '2022-03-04T17:36:44+09:00', 'updated_at': '2022-04-17T01:13:35+09:00', 'id': '6cc714060090160cf2d5', 'likes_count': 6, 'reactions_count': 0, 'stocks_count': 4, 'page_views_count': None, 'organization_url_name': 'elasticsearch_japan', 'comments_count': 1}\n"
     ]
    }
   ],
   "source": [
    "DRIVE_FOLDER = \"./qiita-downloads/\"\n",
    "json_docs = []\n",
    "\n",
    "# 指定されたフォルダ内のファイルを走査\n",
    "for filename in os.listdir(DRIVE_FOLDER):\n",
    "    # ファイルがJSONファイルであることを確認\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(DRIVE_FOLDER, filename)\n",
    "        # JSONファイルを開いてデータを読み取り、dictオブジェクトとして追加\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            tags_names = []\n",
    "            for tag in json_data['tags']:\n",
    "                tags_names.append(tag['name'])\n",
    "            json_data['tags'] = tags_names\n",
    "            json_docs.append(json_data)\n",
    "\n",
    "# 全てのJSONドキュメントがjson_docsリストに格納されました\n",
    "print(f'document count: {len(json_docs)}')\n",
    "print(json_docs[0] if len(json_docs) > 0 else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.様々なサーチを試そう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-A. Elasticsearchのキーワード検索 (BM25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### インデックス作成\n",
    "Kuromojiをアナライザーとして設定したElasticsearchのインデックスを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'qiita-blog-appsearch' deleted successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'qiita-blog-appsearch'})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    # If it exists, delete the index\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "    print(f\"Index '{INDEX_NAME}' deleted successfully.\")\n",
    "else:\n",
    "    print(f\"Index '{INDEX_NAME}' does not exist.\")\n",
    "\n",
    "es.indices.create(\n",
    "  index=INDEX_NAME,\n",
    "  settings={\n",
    "      \"index\": {\n",
    "          \"number_of_shards\": 1,\n",
    "          \"number_of_replicas\": 1\n",
    "      }\n",
    "  }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kuromojiアナライザの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tg/fg_nnncd3p7__vrf94g5k56m0000gn/T/ipykernel_85025/1274768988.py:76: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es.indices.open(index=INDEX_NAME, request_timeout=60)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.close(index=INDEX_NAME)\n",
    "\n",
    "add_settings = {\n",
    "  \"index\": {\n",
    "    \"analysis\": {\n",
    "      \"char_filter\": {\n",
    "        \"normalize\": {\n",
    "          \"mode\": \"compose\",\n",
    "          \"name\": \"nfkc\",\n",
    "          \"type\": \"icu_normalizer\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "es.indices.put_settings(index=INDEX_NAME, body=add_settings)\n",
    "\n",
    "add_settings = {\n",
    "  \"index\": {\n",
    "    \"analysis\": {\n",
    "      \"tokenizer\": {\n",
    "        \"ja_kuromoji_tokenizer\": {\n",
    "          \"mode\": \"search\",\n",
    "          \"discard_compound_token\": \"true\",\n",
    "          \"type\": \"kuromoji_tokenizer\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "es.indices.put_settings(index=INDEX_NAME, body=add_settings)\n",
    "\n",
    "# Define the new settings you want to apply\n",
    "add_settings = {\n",
    "  \"index\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"ja_kuromoji_index_analyzer\": {\n",
    "          \"filter\": [\n",
    "            \"kuromoji_baseform\",\n",
    "            \"kuromoji_part_of_speech\",\n",
    "            \"cjk_width\",\n",
    "            \"ja_stop\",\n",
    "            \"kuromoji_stemmer\",\n",
    "            \"lowercase\"\n",
    "          ],\n",
    "          \"char_filter\": [\n",
    "            \"normalize\"\n",
    "          ],\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"ja_kuromoji_tokenizer\"\n",
    "        },\n",
    "        \"ja_kuromoji_search_analyzer\": {\n",
    "          \"filter\": [\n",
    "            \"kuromoji_baseform\",\n",
    "            \"kuromoji_part_of_speech\",\n",
    "            \"cjk_width\",\n",
    "            \"ja_stop\",\n",
    "            \"kuromoji_stemmer\",\n",
    "            \"lowercase\"\n",
    "          ],\n",
    "          \"char_filter\": [\n",
    "            \"normalize\"\n",
    "          ],\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"ja_kuromoji_tokenizer\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "es.indices.put_settings(index=INDEX_NAME, body=add_settings)\n",
    "\n",
    "es.indices.open(index=INDEX_NAME, request_timeout=60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### App Search用のアナライザの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tg/fg_nnncd3p7__vrf94g5k56m0000gn/T/ipykernel_6510/301467088.py:175: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use the 'settings' parameter. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.indices.put_settings(index=INDEX_NAME, body=add_settings)\n",
      "/var/folders/tg/fg_nnncd3p7__vrf94g5k56m0000gn/T/ipykernel_6510/301467088.py:176: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es.indices.open(index=INDEX_NAME, request_timeout=60)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.close(index=INDEX_NAME)\n",
    "add_settings = {\n",
    "  \"index\": {\n",
    "    \"analysis\": {\n",
    "        \"filter\": {\n",
    "          \"front_ngram\": {\n",
    "            \"type\": \"edge_ngram\",\n",
    "            \"min_gram\": \"1\",\n",
    "            \"max_gram\": \"12\"\n",
    "          },\n",
    "          \"bigram_joiner\": {\n",
    "            \"max_shingle_size\": \"2\",\n",
    "            \"token_separator\": \"\",\n",
    "            \"output_unigrams\": \"false\",\n",
    "            \"type\": \"shingle\"\n",
    "          },\n",
    "          \"bigram_max_size\": {\n",
    "            \"type\": \"length\",\n",
    "            \"max\": \"16\",\n",
    "            \"min\": \"0\"\n",
    "          },\n",
    "          \"bigram_joiner_unigrams\": {\n",
    "            \"max_shingle_size\": \"2\",\n",
    "            \"token_separator\": \"\",\n",
    "            \"output_unigrams\": \"true\",\n",
    "            \"type\": \"shingle\"\n",
    "          },\n",
    "          \"delimiter\": {\n",
    "            \"split_on_numerics\": \"true\",\n",
    "            \"generate_word_parts\": \"true\",\n",
    "            \"preserve_original\": \"false\",\n",
    "            \"catenate_words\": \"true\",\n",
    "            \"generate_number_parts\": \"true\",\n",
    "            \"catenate_all\": \"true\",\n",
    "            \"split_on_case_change\": \"true\",\n",
    "            \"type\": \"word_delimiter_graph\",\n",
    "            \"catenate_numbers\": \"true\",\n",
    "            \"stem_english_possessive\": \"true\"\n",
    "          },\n",
    "          \"ja-stop-words-filter\": {\n",
    "            \"type\": \"stop\",\n",
    "            \"stopwords\": \"_japanese_\"\n",
    "          },\n",
    "          \"ja-stem-filter\": {\n",
    "            \"type\": \"kuromoji_stemmer\",\n",
    "            \"minimum_length\": 4\n",
    "          }\n",
    "        },\n",
    "        \"analyzer\": {\n",
    "          \"i_prefix\": {\n",
    "            \"filter\": [\n",
    "              \"front_ngram\"\n",
    "            ],\n",
    "            \"tokenizer\": \"kuromoji_tokenizer\"\n",
    "          },\n",
    "          \"iq_text_delimiter\": {\n",
    "            \"filter\": [\n",
    "              \"delimiter\",\n",
    "              \"kuromoji_baseform\",\n",
    "              \"kuromoji_part_of_speech\",\n",
    "              \"cjk_width\",\n",
    "              \"ja_stop\",\n",
    "              \"kuromoji_stemmer\",\n",
    "              \"lowercase\"\n",
    "            ],\n",
    "            \"char_filter\": [\n",
    "              \"normalize\"\n",
    "            ],\n",
    "            \"tokenizer\": \"whitespace\"\n",
    "          },\n",
    "          \"q_prefix\": {\n",
    "            \"filter\": [\n",
    "              \"front_ngram\"\n",
    "            ],\n",
    "            \"tokenizer\": \"kuromoji_tokenizer\"\n",
    "          },\n",
    "          \"iq_text_base\": {\n",
    "            \"filter\": [\n",
    "              \"kuromoji_baseform\",\n",
    "              \"kuromoji_part_of_speech\",\n",
    "              \"cjk_width\",\n",
    "              \"ja_stop\",\n",
    "              \"kuromoji_stemmer\",\n",
    "              \"lowercase\"\n",
    "            ],\n",
    "            \"char_filter\": [\n",
    "              \"normalize\"\n",
    "            ],\n",
    "            \"tokenizer\": \"kuromoji_tokenizer\"\n",
    "          },\n",
    "          \"iq_text_stem\": {\n",
    "            \"filter\": [\n",
    "              \"kuromoji_baseform\",\n",
    "              \"kuromoji_part_of_speech\",\n",
    "              \"cjk_width\",\n",
    "              \"ja_stop\",\n",
    "              \"kuromoji_stemmer\",\n",
    "              \"lowercase\"\n",
    "            ],\n",
    "            \"char_filter\": [\n",
    "              \"normalize\"\n",
    "            ],\n",
    "            \"tokenizer\": \"kuromoji_tokenizer\"\n",
    "          },\n",
    "          \"i_text_bigram\": {\n",
    "            \"filter\": [\n",
    "              \"delimiter\",\n",
    "              \"kuromoji_baseform\",\n",
    "              \"kuromoji_part_of_speech\",\n",
    "              \"cjk_width\",\n",
    "              \"ja_stop\",\n",
    "              \"kuromoji_stemmer\",\n",
    "              \"lowercase\",\n",
    "              \"bigram_joiner\",\n",
    "              \"bigram_max_size\"\n",
    "            ],\n",
    "            \"char_filter\": [\n",
    "              \"normalize\"\n",
    "            ],\n",
    "            \"tokenizer\": \"kuromoji_tokenizer\"\n",
    "          },\n",
    "          \"q_text_bigram\": {\n",
    "            \"filter\": [\n",
    "              \"delimiter\",\n",
    "              \"kuromoji_baseform\",\n",
    "              \"kuromoji_part_of_speech\",\n",
    "              \"cjk_width\",\n",
    "              \"ja_stop\",\n",
    "              \"kuromoji_stemmer\",\n",
    "              \"lowercase\",\n",
    "              \"bigram_joiner_unigrams\",\n",
    "              \"bigram_max_size\"\n",
    "            ],\n",
    "            \"char_filter\": [\n",
    "              \"normalize\"\n",
    "            ],\n",
    "            \"tokenizer\": \"kuromoji_tokenizer\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "es.indices.put_settings(index=INDEX_NAME, body=add_settings)\n",
    "es.indices.open(index=INDEX_NAME, request_timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tg/fg_nnncd3p7__vrf94g5k56m0000gn/T/ipykernel_6510/1863774079.py:1: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es.indices.close(index=INDEX_NAME, request_timeout=60)\n",
      "/var/folders/tg/fg_nnncd3p7__vrf94g5k56m0000gn/T/ipykernel_6510/1863774079.py:103: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  es.indices.put_mapping(index=INDEX_NAME, body=add_mapping)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.close(index=INDEX_NAME, request_timeout=60)\n",
    "\n",
    "add_mapping = {\n",
    "  \"properties\": {\n",
    "    \"title\": {\n",
    "      \"type\": \"text\",\n",
    "      \"search_analyzer\": \"ja_kuromoji_search_analyzer\",\n",
    "      \"analyzer\": \"ja_kuromoji_index_analyzer\",\n",
    "      \"fields\": {\n",
    "        \"prefix\": {\n",
    "          \"search_analyzer\": \"ja_kuromoji_search_analyzer\",\n",
    "          \"analyzer\": \"ja_kuromoji_index_analyzer\",\n",
    "          \"type\": \"text\",\n",
    "          \"index_options\": \"docs\"\n",
    "        },\n",
    "        \"delimiter\": {\n",
    "          \"analyzer\": \"iq_text_delimiter\",\n",
    "          \"type\": \"text\",\n",
    "          \"index_options\": \"freqs\"\n",
    "        },\n",
    "        \"joined\": {\n",
    "          \"search_analyzer\": \"q_text_bigram\",\n",
    "          \"analyzer\": \"i_text_bigram\",\n",
    "          \"type\": \"text\",\n",
    "          \"index_options\": \"freqs\"\n",
    "        },\n",
    "        \"stem\": {\n",
    "          \"analyzer\": \"iq_text_stem\",\n",
    "          \"type\": \"text\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"body\": {\n",
    "      \"type\": \"text\",\n",
    "      \"search_analyzer\": \"ja_kuromoji_search_analyzer\",\n",
    "      \"analyzer\": \"ja_kuromoji_index_analyzer\",\n",
    "      \"fields\": {\n",
    "        \"prefix\": {\n",
    "          \"search_analyzer\": \"ja_kuromoji_search_analyzer\",\n",
    "          \"analyzer\": \"ja_kuromoji_index_analyzer\",\n",
    "          \"type\": \"text\",\n",
    "          \"index_options\": \"docs\"\n",
    "        },\n",
    "        \"delimiter\": {\n",
    "          \"analyzer\": \"iq_text_delimiter\",\n",
    "          \"type\": \"text\",\n",
    "          \"index_options\": \"freqs\"\n",
    "        },\n",
    "        \"joined\": {\n",
    "          \"search_analyzer\": \"q_text_bigram\",\n",
    "          \"analyzer\": \"i_text_bigram\",\n",
    "          \"type\": \"text\",\n",
    "          \"index_options\": \"freqs\"\n",
    "        },\n",
    "        \"stem\": {\n",
    "          \"analyzer\": \"iq_text_stem\",\n",
    "          \"type\": \"text\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"seq_num\": {\n",
    "      \"type\": \"long\"\n",
    "    },\n",
    "    \"source\": {\n",
    "      \"type\": \"keyword\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"type\": \"keyword\"\n",
    "    },\n",
    "    \"url\": {\n",
    "      \"type\": \"keyword\"\n",
    "    },\n",
    "    \"id\": {\n",
    "      \"type\": \"keyword\"\n",
    "    },\n",
    "    \"created_at\": {\n",
    "      \"type\": \"date\"\n",
    "    },\n",
    "    \"updated_at\": {\n",
    "      \"type\": \"date\"\n",
    "    },\n",
    "    \"likes_count\": {\n",
    "      \"type\": \"long\"\n",
    "    },\n",
    "    \"reactions_count\": {\n",
    "      \"type\": \"date\"\n",
    "    },\n",
    "    \"stocks_count\": {\n",
    "      \"type\": \"long\"\n",
    "    },\n",
    "    \"page_views_count\": {\n",
    "      \"type\": \"long\"\n",
    "    },\n",
    "    \"comments_count\": {\n",
    "      \"type\": \"long\"\n",
    "    },\n",
    "    \"organization_url_name\": {\n",
    "      \"type\": \"keyword\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "es.indices.put_mapping(index=INDEX_NAME, body=add_mapping)\n",
    "\n",
    "es.indices.open(index=INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### インジェスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tg/fg_nnncd3p7__vrf94g5k56m0000gn/T/ipykernel_6510/1914225579.py:5: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  es.delete_by_query(index=INDEX_NAME, body={\"query\": {\"match_all\": {}}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Elastic Stack 8.0 の NLP で日本語センチメント分析を試してみた - 後編'}\n",
      "{'title': 'ElasticsearchでRAG (Retrieval Augmented Generation) を試す'}\n",
      "{'title': 'Elastic Stack 8.0 の NLP で日本語センチメント分析を試してみた - 前編'}\n",
      "{'title': 'Elastic Observability による Kubernetes クラスタの管理'}\n",
      "{'title': 'Elasticsearchで日付周りをPainlessを使ってうまい具合にハンドリングする'}\n",
      "{'title': '[v8.5版] ElasticsearchとKibanaとElastic Agentの最速インストール手順 (試用環境として）'}\n",
      "{'title': 'Lookup Runtime Field\\u3000〜Elasticsearch 8.2 新機能〜'}\n",
      "{'title': 'ElasticsearchのFrozenデータティアにデータが入るのをテストしてみた (2)'}\n",
      "{'title': 'Elasticsearchのマシン・ラーニング異常検知の動きを理解する(3) [変更設定編]'}\n",
      "{'title': 'Elasticsearchにカスタム時系列データを取り込む (Elastic Agent編)'}\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "# 前の実行で残っているDocumentはクリアしてからインジェストします\n",
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    es.delete_by_query(index=INDEX_NAME, body={\"query\": {\"match_all\": {}}})\n",
    "\n",
    "index_docs = []\n",
    "for doc in json_docs:\n",
    "    # doc_json['_run_ml_inference'] = True\n",
    "    index_docs.append({\n",
    "        \"_index\": INDEX_NAME,\n",
    "        \"_source\": doc,\n",
    "    })\n",
    "\n",
    "try:\n",
    "    r = helpers.bulk(es, index_docs)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise e\n",
    "\n",
    "response = es.search(index=INDEX_NAME, query={\"match_all\": {}}, source=[\"title\"])\n",
    "for hit in response['hits']['hits']:\n",
    "    print(hit['_source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### App Seach用のALIASの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qiita-blog-appsearch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'search-qiita-blog-appsearch'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALIAS=f\"search-{INDEX_NAME}\"\n",
    "es.indices.put_alias(index=INDEX_NAME, name=ALIAS)\n",
    "ALIAS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
